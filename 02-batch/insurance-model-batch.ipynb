{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd3fb4e-ab5c-486b-a8e0-85f6a9d17292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "from databricks.feature_engineering import FeatureEngineeringClient\nimport mlflow.pyfunc\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import IntegerType, FloatType, DoubleType\n\nclass HealthcareBatchInference:\n    \"\"\"\n    Batch inference class aligned with feature engineering and training.\n    Uses dim_patients table and customer_id key mapping.\n    \"\"\"\n    \n    def __init__(self, model_name=\"juan_dev.healthcare_data.insurance_model\", model_alias=\"champion\"):\n        self.model_name = model_name\n        self.model_alias = model_alias\n        self.fe = FeatureEngineeringClient()\n        \n        # Spark optimization for batch processing\n        spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n        spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n        spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\")\n    \n    def run_batch_inference(self, input_table=\"juan_dev.healthcare_data.dim_patients\", output_table=None):\n        \"\"\"Execute batch inference using feature engineering integration\"\"\"\n        \n        try:\n            # Load model from Unity Catalog\n            model_uri = f\"models:/{self.model_name}@{self.model_alias}\"\n            \n            # Get model version info\n            client = mlflow.MlflowClient()\n            model_version_info = client.get_model_version_by_alias(self.model_name, self.model_alias)\n            model_version = model_version_info.version\n            \n            print(f\"Loading model version {model_version} from {model_uri}\")\n            \n            # Load input data from dim_patients - filter for current records (same as training)\n            input_df = spark.table(input_table).filter(col(\"is_current_record\") == True)\n            print(f\"Input data shape: {input_df.count()} rows, {len(input_df.columns)} columns\")\n            \n            # Prepare data by creating customer_id key\n            # The feature engineering client will automatically join features from the feature table\n            input_df_prepared = (\n                input_df\n                .withColumn(\"customer_id\", col(\"patient_natural_key\"))\n            )\n            \n            print(\"Data preparation completed successfully\")\n            print(f\"Customer ID sample: {[row.customer_id for row in input_df_prepared.select('customer_id').take(3)]}\")\n            \n            # Batch scoring with feature engineering integration\n            # The fe.score_batch will automatically:\n            # 1. Join with feature table (ml_insurance_features) using customer_id\n            # 2. Get all required features (age_risk_score, smoking_impact, etc.)\n            # 3. Apply the model's preprocessing pipeline\n            # 4. Generate predictions\n            print(\"Starting batch scoring with feature engineering integration...\")\n            \n            predictions_df = self.fe.score_batch(\n                df=input_df_prepared,\n                model_uri=model_uri\n            )\n            \n            print(\"Batch scoring completed successfully!\")\n            print(f\"Predictions shape: {predictions_df.count()} rows\")\n            print(f\"Prediction columns: {predictions_df.columns}\")\n            \n            # Add business logic and metadata\n            final_predictions = (\n                predictions_df\n                .withColumn(\"prediction_timestamp\", current_timestamp())\n                .withColumn(\"model_version\", lit(model_version))\n                .withColumn(\"model_name\", lit(self.model_name))\n                \n                # Business rule: minimum risk score threshold\n                .withColumn(\"adjusted_prediction\", \n                           expr(\"GREATEST(prediction, 10)\"))  # Minimum risk score of 10\n                \n                # Risk categorization for business use (adjusted for health risk scores 0-100)\n                .withColumn(\"risk_category\",\n                           expr(\"CASE WHEN adjusted_prediction < 30 THEN 'low' \" +\n                                \"WHEN adjusted_prediction < 60 THEN 'medium' \" +\n                                \"WHEN adjusted_prediction < 85 THEN 'high' \" +\n                                \"ELSE 'critical' END\"))\n                \n                # Confidence intervals (approximate business rules)\n                .withColumn(\"prediction_lower_bound\", \n                           expr(\"adjusted_prediction * 0.90\"))\n                .withColumn(\"prediction_upper_bound\", \n                           expr(\"adjusted_prediction * 1.10\"))\n                \n                # Add risk flags for business decision making\n                .withColumn(\"high_risk_patient\",\n                           expr(\"adjusted_prediction > 75 OR risk_category = 'critical'\"))\n                .withColumn(\"requires_review\", \n                           expr(\"adjusted_prediction > 90\"))\n            )\n            \n            # Display results for inspection\n            print(\"Sample predictions:\")\n            final_predictions.select(\n                \"customer_id\", \"prediction\", \"adjusted_prediction\", \"risk_category\",\n                \"high_risk_patient\", \"requires_review\"\n            ).show(10)\n            \n            # Save results if output table specified\n            if output_table:\n                print(f\"Saving results to {output_table}...\")\n                (final_predictions\n                 .write\n                 .mode(\"overwrite\")\n                 .option(\"overwriteSchema\", \"true\")\n                 .saveAsTable(output_table))\n                print(\"Results saved successfully!\")\n            \n            # Log batch inference metrics for monitoring\n            with mlflow.start_run(run_name=\"batch_inference_health_risk\"):\n                inference_count = final_predictions.count()\n                \n                # Calculate business metrics (adjusted for health risk prediction)\n                avg_prediction = final_predictions.agg(avg(\"adjusted_prediction\")).collect()[0][0]\n                high_risk_count = final_predictions.filter(col(\"high_risk_patient\") == True).count()\n                requires_review_count = final_predictions.filter(col(\"requires_review\") == True).count()\n                \n                # Risk category distribution\n                risk_distribution = final_predictions.groupBy(\"risk_category\").count().collect()\n                risk_dist_dict = {row.risk_category: row['count'] for row in risk_distribution}\n                \n                mlflow.log_metrics({\n                    \"batch_inference_count\": inference_count,\n                    \"average_predicted_risk_score\": avg_prediction,\n                    \"high_risk_patient_count\": high_risk_count,\n                    \"requires_review_count\": requires_review_count,\n                    \"high_risk_percentage\": (high_risk_count / inference_count) * 100,\n                    \"model_version\": float(model_version),\n                    **{f\"risk_{k}_count\": v for k, v in risk_dist_dict.items()}\n                })\n                \n                print(f\"Logged metrics - Average predicted risk score: {avg_prediction:.2f}\")\n                print(f\"High risk patients: {high_risk_count} ({high_risk_count/inference_count*100:.1f}%)\")\n                print(f\"Require review: {requires_review_count} ({requires_review_count/inference_count*100:.1f}%)\")\n            \n            return final_predictions\n            \n        except Exception as e:\n            print(f\"Error during batch inference: {str(e)}\")\n            print(\"Troubleshooting steps:\")\n            print(\"1. Check that the model exists and has the 'champion' alias\")\n            print(\"2. Verify input data contains patient_natural_key column\")\n            print(\"3. Ensure feature table ml_insurance_features is accessible\")\n            print(\"4. Check Unity Catalog permissions\")\n            print(\"5. Verify customer_id mapping from patient_natural_key\")\n            import traceback\n            traceback.print_exc()\n            raise e\n\n# Example usage\nprint(\"Initializing batch inference pipeline...\")\n\nbatch_inference = HealthcareBatchInference()\n\nprint(\"Running batch inference on healthcare data using dim_patients table...\")\ntry:\n    results = batch_inference.run_batch_inference(\n        input_table=\"juan_dev.healthcare_data.dim_patients\",\n        output_table=\"juan_dev.healthcare_data.ml_patient_predictions\"\n    )\n    \n    print(\"\\n=== Batch Inference Summary ===\")\n    print(f\"Successfully processed {results.count()} records\")\n    print(\"Results saved to predictions table\")\n    \n    # Show sample results\n    print(\"\\n=== Sample Predictions ===\")\n    results.select(\n        \"customer_id\", \n        \"adjusted_prediction\",\n        \"risk_category\",\n        \"high_risk_patient\",\n        \"requires_review\"\n    ).show(5)\n    \n    # Show risk distribution\n    print(\"\\n=== Risk Category Distribution ===\")\n    results.groupBy(\"risk_category\").count().orderBy(\"risk_category\").show()\n    \nexcept Exception as e:\n    print(f\"Batch inference failed: {e}\")\n    print(\"Please check the error message above and follow troubleshooting steps\")"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78d3d68f-d2c8-4315-b9b2-9fa04e03c66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4665559814347413,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "insurance-model-batch",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}