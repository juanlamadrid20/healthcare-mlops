{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Healthcare ML Model Monitoring with Databricks Lakehouse Monitoring\n",
        "\n",
        "This notebook implements comprehensive monitoring for the healthcare insurance risk prediction model using native Databricks Lakehouse Monitoring APIs.\n",
        "\n",
        "## Monitoring Architecture\n",
        "\n",
        "1. **Inference Monitoring** - Tracks model predictions and prediction drift\n",
        "2. **Feature Store Monitoring** - Monitors feature quality and distribution\n",
        "3. **Baseline Data Monitoring** - Tracks upstream data quality\n",
        "4. **Custom Healthcare Metrics** - Fairness, bias, and business KPIs\n",
        "\n",
        "## Setup Requirements\n",
        "\n",
        "- Databricks SDK >= 0.28.0\n",
        "- Unity Catalog tables populated with data\n",
        "- Appropriate permissions to create monitors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install/upgrade databricks-sdk\n",
        "%pip install \"databricks-sdk>=0.28.0\" --quiet\n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialize Monitoring Infrastructure\n",
        "\n",
        "**Note**: If you've updated the monitoring modules, restart Python to reload them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add monitoring module to path\n",
        "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
        "project_root = os.path.dirname(os.path.dirname(notebook_path))\n",
        "monitoring_path = os.path.join(project_root, \"03-monitoring\")\n",
        "\n",
        "if monitoring_path not in sys.path:\n",
        "    sys.path.insert(0, monitoring_path)\n",
        "\n",
        "# Import monitoring modules\n",
        "from lakehouse_monitoring import HealthcareMonitorManager, MonitorRefreshManager, MonitorAnalyzer\n",
        "from custom_metrics import FairnessMetricsCalculator, BusinessMetricsCalculator, DriftDetector, calculate_all_custom_metrics\n",
        "\n",
        "print(\"✓ Monitoring modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CATALOG = \"juan_dev\"\n",
        "SCHEMA = \"healthcare_data\"\n",
        "\n",
        "# Get current user email for notifications\n",
        "user_email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Catalog: {CATALOG}\")\n",
        "print(f\"  Schema: {SCHEMA}\")\n",
        "print(f\"  User: {user_email}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Lakehouse Monitors\n",
        "\n",
        "Create monitors for all three tables in the monitoring architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the monitor manager\n",
        "monitor_manager = HealthcareMonitorManager(\n",
        "    catalog=CATALOG,\n",
        "    schema=SCHEMA,\n",
        "    user_email=user_email\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create all monitors with scheduled daily refresh at 6 AM UTC\n",
        "import json\n",
        "\n",
        "monitor_results = monitor_manager.create_all_monitors(\n",
        "    schedule_cron=\"0 0 6 * * ?\",  # Daily at 6 AM UTC\n",
        "    notification_emails=[user_email]\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"\\nMonitor Creation Results:\")\n",
        "print(json.dumps(monitor_results, indent=2, default=str))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Trigger Initial Monitor Refresh\n",
        "\n",
        "Manually trigger the first refresh to generate initial metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize refresh manager\n",
        "refresh_manager = MonitorRefreshManager(monitor_manager)\n",
        "\n",
        "# Refresh all monitors and wait for completion\n",
        "refresh_results = refresh_manager.refresh_all_monitors(\n",
        "    wait_for_completion=True,\n",
        "    timeout_seconds=1800  # 30 minutes timeout per monitor\n",
        ")\n",
        "\n",
        "print(\"\\nRefresh Results:\")\n",
        "print(json.dumps(refresh_results, indent=2, default=str))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Calculate Custom Healthcare Metrics\n",
        "\n",
        "Compute fairness, bias, and business metrics specific to healthcare.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate all custom metrics\n",
        "custom_metrics = calculate_all_custom_metrics(\n",
        "    spark=spark,\n",
        "    predictions_table=f\"{CATALOG}.{SCHEMA}.ml_patient_predictions\",\n",
        "    baseline_table=f\"{CATALOG}.{SCHEMA}.dim_patients\",  # Use dim_patients for baseline\n",
        "    output_schema=f\"{CATALOG}.{SCHEMA}\",\n",
        "    catalog=CATALOG,\n",
        "    schema=SCHEMA\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Custom metrics calculated and saved to Unity Catalog\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display Fairness, Business, and Drift Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display all custom metrics\n",
        "fairness_df = custom_metrics['fairness']\n",
        "business_df = custom_metrics['business']\n",
        "drift_df = custom_metrics['drift']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FAIRNESS METRICS\")\n",
        "print(\"=\" * 80)\n",
        "display(fairness_df)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BUSINESS METRICS\")\n",
        "print(\"=\" * 80)\n",
        "display(business_df)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DRIFT ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "display(drift_df)\n",
        "\n",
        "# Check for alerts\n",
        "if fairness_df['fairness_threshold_violation'].any():\n",
        "    print(\"\\n⚠️ FAIRNESS ALERT: Threshold violations detected!\")\n",
        "    \n",
        "significant_drift = drift_df[drift_df['requires_action'] == True]\n",
        "if len(significant_drift) > 0:\n",
        "    print(\"\\n⚠️ DRIFT ALERT: Significant drift detected!\")\n",
        "    print(\"Columns requiring attention:\")\n",
        "    for _, row in significant_drift.iterrows():\n",
        "        print(f\"  - {row['column_name']}: PSI = {row['psi_score']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Query Lakehouse Monitor Metrics\n",
        "\n",
        "Access the metrics tables generated by Databricks Lakehouse Monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize analyzer\n",
        "analyzer = MonitorAnalyzer(monitor_manager, spark)\n",
        "\n",
        "# Get profile metrics for inference table\n",
        "inference_profile = analyzer.get_profile_metrics(\n",
        "    monitor_manager.inference_table,\n",
        "    limit=20\n",
        ")\n",
        "\n",
        "if inference_profile:\n",
        "    print(\"Inference Profile Metrics:\")\n",
        "    display(inference_profile)\n",
        "else:\n",
        "    print(\"No profile metrics available yet. Ensure monitor has been refreshed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Executive Monitoring Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary\n",
        "summary = analyzer.generate_monitoring_summary()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"HEALTHCARE ML MODEL MONITORING SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTimestamp: {summary['timestamp']}\")\n",
        "print(f\"\\nMonitor Status:\")\n",
        "\n",
        "for monitor_name, monitor_info in summary['monitors'].items():\n",
        "    print(f\"\\n{monitor_name.upper()}:\")\n",
        "    for key, value in monitor_info.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Create summary dashboard data\n",
        "summary_data = {\n",
        "    \"metric_timestamp\": pd.Timestamp.now(),\n",
        "    \"total_predictions\": int(business_df['total_predictions'].iloc[0]),\n",
        "    \"high_risk_percentage\": float(business_df['high_risk_percentage'].iloc[0]),\n",
        "    \"daily_avg_predictions\": float(business_df['daily_avg_predictions'].iloc[0]),\n",
        "    \"mean_prediction\": float(business_df['mean_prediction'].iloc[0]),\n",
        "    \"fairness_violations\": int(fairness_df['fairness_threshold_violation'].sum()),\n",
        "    \"significant_drift_count\": int(drift_df['requires_action'].sum()),\n",
        "    \"monitors_active\": len([m for m in summary['monitors'].values() if m.get('status') != 'error'])\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame([summary_data])\n",
        "print(\"\\n\\nMonitoring Summary:\")\n",
        "display(summary_df)\n",
        "\n",
        "# Save summary to Unity Catalog\n",
        "summary_spark_df = spark.createDataFrame(summary_df)\n",
        "summary_spark_df.write.mode(\"append\").saveAsTable(\n",
        "    f\"{CATALOG}.{SCHEMA}.monitoring_summary_history\"\n",
        ")\n",
        "print(f\"\\n✓ Summary saved to {CATALOG}.{SCHEMA}.monitoring_summary_history\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "\n",
        "1. ✓ Created Lakehouse Monitors for inference, features, and baseline data\n",
        "2. ✓ Triggered initial monitor refreshes\n",
        "3. ✓ Calculated custom healthcare-specific metrics (fairness, business, drift)\n",
        "4. ✓ Analyzed monitoring results and generated alerts\n",
        "5. ✓ Saved monitoring summary for historical tracking\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Review the auto-generated Databricks dashboards in the monitor assets directories\n",
        "- Set up additional custom alerts based on your requirements\n",
        "- Schedule this notebook to run daily for continuous monitoring\n",
        "- Integrate monitoring metrics into your MLOps pipeline\n",
        "\n",
        "### Accessing Monitor Dashboards\n",
        "\n",
        "Databricks automatically creates interactive dashboards for each monitor. Access them via:\n",
        "1. Navigate to the table in Catalog Explorer\n",
        "2. Click on the \"Quality\" tab\n",
        "3. View the monitoring dashboard\n",
        "\n",
        "Or find them in the workspace at:\n",
        "- `/Workspace/Users/{your_email}/databricks_lakehouse_monitoring/`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
