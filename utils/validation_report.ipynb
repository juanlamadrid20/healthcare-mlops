{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Insurance Risk Model - Executive Validation Report\n",
    "\n",
    "**Purpose**: Comprehensive validation report combining technical, business, and clinical validation for stakeholder review.\n",
    "\n",
    "**Audience**: Executives, Product Managers, Compliance Officers, Clinical Stakeholders\n",
    "\n",
    "**Report Sections**:\n",
    "1. Executive Summary\n",
    "2. Technical Validation\n",
    "3. Business Performance\n",
    "4. Clinical Relevance\n",
    "5. Compliance & Governance\n",
    "6. Recommendations & Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and configuration\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Configure parameters\n",
    "dbutils.widgets.text(\"catalog\", \"juan_dev\", \"Unity Catalog\")\n",
    "dbutils.widgets.text(\"ml_schema\", \"healthcare_data\", \"ML Schema\")\n",
    "dbutils.widgets.text(\"model_name\", \"insurance_model\", \"Model Name\")\n",
    "dbutils.widgets.text(\"report_period_days\", \"30\", \"Report Period (days)\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "ml_schema = dbutils.widgets.get(\"ml_schema\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "report_period = int(dbutils.widgets.get(\"report_period_days\"))\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Table references\n",
    "predictions_table = f\"{catalog}.{ml_schema}.ml_patient_predictions\"\n",
    "full_model_name = f\"{catalog}.{ml_schema}.{model_name}\"\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"HEALTHCARE INSURANCE RISK MODEL - EXECUTIVE VALIDATION REPORT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Model: {full_model_name}\")\n",
    "print(f\"Report Period: Last {report_period} days\")\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Executive Summary\n",
    "\n",
    "High-level overview of model performance, business impact, and key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"EXECUTIVE SUMMARY\")\nprint(\"=\"*80)\n\n# Load predictions data\npredictions_df = spark.table(predictions_table)\ncutoff_date = (datetime.now() - timedelta(days=report_period)).strftime('%Y-%m-%d')\nrecent_predictions = predictions_df.filter(col(\"prediction_timestamp\") >= cutoff_date)\n\n# Key metrics for executive summary\nexec_summary = recent_predictions.agg(\n    countDistinct(\"patient_natural_key\").alias(\"unique_patients\"),\n    count(\"*\").alias(\"total_predictions\"),\n    avg(\"adjusted_prediction\").alias(\"avg_risk_score\"),\n    sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)).alias(\"high_risk_count\")\n).collect()[0]\n\nhigh_risk_pct = (exec_summary.high_risk_count / exec_summary.total_predictions) * 100\n\n# Business impact estimates\navg_intervention_cost = 500\navg_prevented_claim = 5000\nsuccess_rate = 0.30\n\nintervention_costs = exec_summary.high_risk_count * avg_intervention_cost\nprevented_value = exec_summary.high_risk_count * success_rate * avg_prevented_claim\nnet_benefit = prevented_value - intervention_costs\nroi = (net_benefit / intervention_costs * 100) if intervention_costs > 0 else 0\n\nprint(f\"\\nüìä Key Performance Indicators:\")\nprint(f\"  ‚Ä¢ Patients Evaluated: {exec_summary.unique_patients:,}\")\nprint(f\"  ‚Ä¢ Total Risk Assessments: {exec_summary.total_predictions:,}\")\nprint(f\"  ‚Ä¢ Average Risk Score: {exec_summary.avg_risk_score:.1f}/100\")\nprint(f\"  ‚Ä¢ High-Risk Patients Identified: {exec_summary.high_risk_count:,} ({high_risk_pct:.1f}%)\")\n\nprint(f\"\\nüí∞ Estimated Business Impact:\")\nprint(f\"  ‚Ä¢ Intervention Costs: ${intervention_costs:,.0f}\")\nprint(f\"  ‚Ä¢ Prevented Claims Value: ${prevented_value:,.0f}\")\nprint(f\"  ‚Ä¢ Net Benefit: ${net_benefit:,.0f}\")\nprint(f\"  ‚Ä¢ Return on Investment: {roi:.0f}%\")\n\n# Model status\ntry:\n    champion_info = client.get_model_version_by_alias(full_model_name, \"champion\")\n    run_data = client.get_run(champion_info.run_id)\n    r2_score = run_data.data.metrics.get(\"r2_score\", 0)\n    mae = run_data.data.metrics.get(\"mean_absolute_error\", 0)\n    \n    print(f\"\\nü§ñ Model Performance:\")\n    print(f\"  ‚Ä¢ Champion Version: {champion_info.version}\")\n    print(f\"  ‚Ä¢ Model Accuracy (R¬≤): {r2_score:.3f}\")\n    print(f\"  ‚Ä¢ Prediction Error (MAE): {mae:.2f}\")\n    print(f\"  ‚Ä¢ Status: {champion_info.status}\")\nexcept Exception as e:\n    print(f\"\\n‚ö†Ô∏è  Model Status: Unable to retrieve champion model information\")\n\nprint(f\"\\n‚úÖ Overall Assessment:\")\nif roi > 100 and high_risk_pct >= 5 and high_risk_pct <= 25:\n    print(f\"  Model is performing EXCELLENTLY and delivering strong business value.\")\n    overall_status = \"EXCELLENT\"\nelif roi > 50:\n    print(f\"  Model is performing WELL and delivering positive business value.\")\n    overall_status = \"GOOD\"\nelif roi > 0:\n    print(f\"  Model is ACCEPTABLE but has room for improvement.\")\n    overall_status = \"ACCEPTABLE\"\nelse:\n    print(f\"  Model requires ATTENTION and optimization.\")\n    overall_status = \"NEEDS_IMPROVEMENT\"\n\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Technical Validation\n",
    "\n",
    "Data quality, prediction accuracy, and technical health checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"TECHNICAL VALIDATION\")\nprint(\"=\"*80)\n\n# Run data validation\ndata_validation_result = dbutils.notebook.run(\n    \"./data_validation\",\n    timeout_seconds=300,\n    arguments={\n        \"catalog\": catalog,\n        \"schema\": ml_schema.replace(\"healthcare_data\", \"healthcare_data\"),\n        \"validation_date\": datetime.now().strftime('%Y-%m-%d')\n    }\n)\n\nprint(f\"\\n‚úÖ Data Validation Results:\")\ntry:\n    validation_data = json.loads(data_validation_result)\n    print(f\"  Status: {validation_data.get('status', 'UNKNOWN')}\")\n    if validation_data.get('status') == 'SUCCESS':\n        print(f\"  ‚úì All prerequisite tables validated\")\n        print(f\"  ‚úì Required columns present\")\n        print(f\"  ‚úì Data quality checks passed\")\n    else:\n        print(f\"  ‚ö†Ô∏è  {validation_data.get('message', 'Validation issues detected')}\")\nexcept Exception as e:\n    print(f\"  ‚ÑπÔ∏è  Validation completed with status: {data_validation_result}\")\n\n# Data quality metrics - check if confidence interval columns exist\nhas_confidence_cols = \"confidence_interval_upper\" in recent_predictions.columns\n\nif has_confidence_cols:\n    quality_metrics = recent_predictions.agg(\n        (sum(when(col(\"adjusted_prediction\").isNull(), 1).otherwise(0)) / count(\"*\") * 100).alias(\"null_pct\"),\n        (sum(when(col(\"adjusted_prediction\") < 0, 1).otherwise(0)) / count(\"*\") * 100).alias(\"negative_pct\"),\n        (sum(when(col(\"adjusted_prediction\") > 100, 1).otherwise(0)) / count(\"*\") * 100).alias(\"outlier_pct\"),\n        avg(col(\"confidence_interval_upper\") - col(\"confidence_interval_lower\")).alias(\"avg_ci\")\n    ).collect()[0]\nelse:\n    quality_metrics = recent_predictions.agg(\n        (sum(when(col(\"adjusted_prediction\").isNull(), 1).otherwise(0)) / count(\"*\") * 100).alias(\"null_pct\"),\n        (sum(when(col(\"adjusted_prediction\") < 0, 1).otherwise(0)) / count(\"*\") * 100).alias(\"negative_pct\"),\n        (sum(when(col(\"adjusted_prediction\") > 100, 1).otherwise(0)) / count(\"*\") * 100).alias(\"outlier_pct\")\n    ).collect()[0]\n\nprint(f\"\\nüìä Prediction Quality Metrics:\")\nprint(f\"  ‚Ä¢ Data Completeness: {100 - quality_metrics.null_pct:.2f}%\")\nprint(f\"  ‚Ä¢ Outliers: {quality_metrics.outlier_pct:.2f}%\")\nif has_confidence_cols:\n    print(f\"  ‚Ä¢ Avg Confidence Interval: ¬±{quality_metrics.avg_ci:.2f}\")\n\nif quality_metrics.null_pct < 1 and quality_metrics.outlier_pct < 1:\n    print(f\"  ‚úÖ Excellent data quality\")\nelif quality_metrics.null_pct < 5:\n    print(f\"  ‚úì Acceptable data quality\")\nelse:\n    print(f\"  ‚ö†Ô∏è  Data quality requires attention\")\n\n# Prediction volume trends\nprint(f\"\\nüìà Prediction Volume Trends:\")\ndaily_volume = recent_predictions.groupBy(\n    date_format(col(\"prediction_timestamp\"), \"yyyy-MM-dd\").alias(\"date\")\n).agg(\n    count(\"*\").alias(\"prediction_count\")\n).orderBy(\"date\", ascending=False).limit(7).collect()\n\nprint(f\"  Last 7 days:\")\nfor row in daily_volume:\n    print(f\"    {row.date}: {row.prediction_count:,} predictions\")\n\n# Use builtins.sum to avoid conflict with PySpark's sum function\nimport builtins\navg_daily = builtins.sum([row.prediction_count for row in daily_volume]) / len(daily_volume) if daily_volume else 0\nprint(f\"  ‚Ä¢ Average daily volume: {avg_daily:,.0f} predictions\")\n\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Business Performance\n",
    "\n",
    "Risk distribution, high-risk identification, and business KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"BUSINESS PERFORMANCE\")\nprint(\"=\"*80)\n\n# Risk category distribution\nprint(f\"\\nüìä Risk Category Distribution:\")\nrisk_dist = recent_predictions.groupBy(\"risk_category\").agg(\n    count(\"*\").alias(\"count\")\n).orderBy(\"risk_category\").collect()\n\n# Use builtins.sum to avoid conflict with PySpark's sum\nimport builtins\ntotal = builtins.sum([row['count'] for row in risk_dist])\nfor row in risk_dist:\n    pct = (row['count'] / total) * 100\n    bar = \"‚ñà\" * int(pct / 2)  # Visual bar chart\n    print(f\"  {row.risk_category:>10}: {row['count']:>7,} ({pct:>5.1f}%) {bar}\")\n\n# Business KPI validation\nprint(f\"\\n‚úÖ Business KPI Validation:\")\nlow_pct = next((row['count'] / total * 100 for row in risk_dist if row.risk_category == 'low'), 0)\ncritical_pct = next((row['count'] / total * 100 for row in risk_dist if row.risk_category == 'critical'), 0)\n\nprint(f\"  ‚Ä¢ Low-risk patients: {low_pct:.1f}% (Target: >30%)\")\nif low_pct >= 30:\n    print(f\"    ‚úì Healthy baseline population\")\nelse:\n    print(f\"    ‚ö†Ô∏è  Lower than expected - investigate population health\")\n\nprint(f\"  ‚Ä¢ High-risk patients: {high_risk_pct:.1f}% (Target: 5-25%)\")\nif 5 <= high_risk_pct <= 25:\n    print(f\"    ‚úì Appropriate high-risk identification rate\")\nelif high_risk_pct < 5:\n    print(f\"    ‚ö†Ô∏è  May be under-identifying at-risk patients\")\nelse:\n    print(f\"    ‚ö†Ô∏è  May be over-flagging patients (alert fatigue risk)\")\n\nprint(f\"  ‚Ä¢ Critical cases: {critical_pct:.1f}% (Target: <10%)\")\nif critical_pct < 10:\n    print(f\"    ‚úì Critical case rate within acceptable range\")\nelse:\n    print(f\"    ‚ö†Ô∏è  High critical rate - verify population or model calibration\")\n\n# Intervention candidates\nprint(f\"\\nüéØ Intervention Opportunities:\")\nhigh_risk_df = recent_predictions.filter(col(\"high_risk_patient\") == True)\nintervention_breakdown = high_risk_df.groupBy(\"risk_category\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"adjusted_prediction\").alias(\"avg_risk\")\n).orderBy(col(\"avg_risk\").desc()).collect()\n\nprint(f\"  Total intervention candidates: {exec_summary.high_risk_count:,}\")\nfor row in intervention_breakdown:\n    print(f\"    ‚Ä¢ {row.risk_category}: {row['count']:,} patients (avg risk: {row.avg_risk:.1f})\")\n\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clinical Relevance\n",
    "\n",
    "Validate that predictions align with medical knowledge and healthcare best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CLINICAL RELEVANCE VALIDATION\")\nprint(\"=\"*80)\n\n# Smoking impact\nprint(f\"\\nüö¨ Smoking Status Impact:\")\nsmoking_analysis = recent_predictions.groupBy(\"patient_smoking_status\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n    (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\")\n).collect()\n\nfor row in smoking_analysis:\n    print(f\"  {row.patient_smoking_status:>5}: Avg Risk={row.avg_risk:>5.1f}, High-Risk={row.high_risk_pct:>5.1f}%, N={row['count']:>6,}\")\n\nsmoker_risks = {row.patient_smoking_status: row.avg_risk for row in smoking_analysis}\nif \"yes\" in smoker_risks and \"no\" in smoker_risks:\n    if smoker_risks[\"yes\"] > smoker_risks[\"no\"]:\n        impact = smoker_risks[\"yes\"] - smoker_risks[\"no\"]\n        print(f\"  ‚úÖ Smokers have {impact:.1f} points higher risk (clinically valid)\")\n    else:\n        print(f\"  ‚ùå Smoking NOT properly weighted - model calibration issue\")\n\n# Age correlation\nprint(f\"\\nüë¥ Age Category Analysis:\")\nage_analysis = recent_predictions.groupBy(\"patient_age_category\").agg(\n    count(\"*\").alias(\"count\"),\n    avg(\"adjusted_prediction\").alias(\"avg_risk\")\n).orderBy(\"patient_age_category\").collect()\n\nprev_risk = 0\nage_valid = True\nfor row in age_analysis:\n    trend = \"‚Üó\" if row.avg_risk > prev_risk else \"‚Üò\" if row.avg_risk < prev_risk else \"‚Üí\"\n    print(f\"  {row.patient_age_category:>15}: {row.avg_risk:>5.1f} {trend} (N={row['count']:,})\")\n    if row.avg_risk < prev_risk:\n        age_valid = False\n    prev_risk = row.avg_risk\n\nif age_valid:\n    print(f\"  ‚úÖ Age positively correlated with risk (clinically valid)\")\nelse:\n    print(f\"  ‚ö†Ô∏è  Age correlation inconsistent - review model features\")\n\n# BMI impact - check if column exists\nhas_bmi_category = \"bmi_category\" in recent_predictions.columns\nif has_bmi_category:\n    print(f\"\\n‚öñÔ∏è  BMI Category Impact:\")\n    bmi_analysis = recent_predictions.groupBy(\"bmi_category\").agg(\n        count(\"*\").alias(\"count\"),\n        avg(\"adjusted_prediction\").alias(\"avg_risk\")\n    ).orderBy(\"bmi_category\").collect()\n\n    for row in bmi_analysis:\n        print(f\"  {row.bmi_category:>12}: Avg Risk={row.avg_risk:>5.1f} (N={row['count']:,})\")\n\n    bmi_risks = {row.bmi_category: row.avg_risk for row in bmi_analysis}\n    if \"obese\" in bmi_risks and \"normal\" in bmi_risks:\n        if bmi_risks[\"obese\"] > bmi_risks[\"normal\"]:\n            print(f\"  ‚úÖ Obesity correctly associated with higher risk\")\n        else:\n            print(f\"  ‚ö†Ô∏è  BMI relationship unclear - verify feature engineering\")\nelse:\n    bmi_risks = {}\n    print(f\"\\n‚öñÔ∏è  BMI Category: Column not available in predictions table\")\n\nprint(f\"\\nüìã Clinical Validation Summary:\")\nclinical_checks = [\n    (\"Smoking impact\", smoker_risks.get(\"yes\", 0) > smoker_risks.get(\"no\", 0) if \"yes\" in smoker_risks and \"no\" in smoker_risks else False),\n    (\"Age correlation\", age_valid)\n]\nif has_bmi_category:\n    clinical_checks.append((\"BMI impact\", bmi_risks.get(\"obese\", 0) > bmi_risks.get(\"normal\", 0) if \"obese\" in bmi_risks and \"normal\" in bmi_risks else False))\n\nimport builtins\npassed_clinical = builtins.sum([1 for _, result in clinical_checks if result])\nfor check_name, result in clinical_checks:\n    print(f\"  {'‚úì' if result else '‚úó'} {check_name}\")\n\nprint(f\"\\n  Clinical validation score: {passed_clinical}/{len(clinical_checks)} checks passed\")\n\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regional Equity & Fairness\n",
    "\n",
    "Ensure predictions are fair and equitable across demographics and regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"REGIONAL EQUITY & FAIRNESS ANALYSIS\")\nprint(\"=\"*80)\n\n# Import builtins to avoid conflicts with PySpark functions\nimport builtins\n\n# Regional analysis - check if region column exists\nhas_region = \"patient_region\" in recent_predictions.columns or \"region\" in recent_predictions.columns\nregion_col = \"patient_region\" if \"patient_region\" in recent_predictions.columns else \"region\"\n\nif has_region:\n    print(f\"\\nüåé Regional Performance:\")\n    regional_analysis = recent_predictions.groupBy(region_col).agg(\n        count(\"*\").alias(\"count\"),\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        stddev(\"adjusted_prediction\").alias(\"risk_std\"),\n        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\")\n    ).orderBy(region_col).collect()\n\n    print(f\"  {'Region':<15} {'Avg Risk':<12} {'Std Dev':<12} {'High-Risk %':<12} {'Count':<10}\")\n    print(f\"  {'-'*70}\")\n    for row in regional_analysis:\n        region_name = getattr(row, region_col)\n        print(f\"  {region_name:<15} {row.avg_risk:<12.2f} {row.risk_std:<12.2f} {row.high_risk_pct:<12.1f} {row['count']:<10,}\")\n\n    # Calculate regional disparity using builtins to avoid PySpark function conflicts\n    regional_risks = [row.avg_risk for row in regional_analysis]\n    if len(regional_risks) > 1:\n        max_risk = builtins.max(regional_risks)\n        min_risk = builtins.min(regional_risks)\n        disparity = ((max_risk - min_risk) / min_risk) * 100\n        \n        print(f\"\\n  Regional Disparity: {disparity:.1f}%\")\n        if disparity <= 15:\n            print(f\"  ‚úÖ Excellent regional equity (<15% disparity)\")\n        elif disparity <= 25:\n            print(f\"  ‚úì Acceptable regional equity (<25% disparity)\")\n        else:\n            print(f\"  ‚ö†Ô∏è  High regional disparity - investigate for bias\")\n    else:\n        disparity = 0\nelse:\n    print(f\"\\nüåé Regional Performance: Region column not available in predictions table\")\n    regional_risks = []\n    disparity = 0\n\n# Gender equity - check if gender column exists\nhas_gender = \"patient_gender\" in recent_predictions.columns or \"sex\" in recent_predictions.columns\ngender_col = \"patient_gender\" if \"patient_gender\" in recent_predictions.columns else \"sex\"\n\nif has_gender:\n    print(f\"\\n‚öñÔ∏è  Gender Equity:\")\n    gender_analysis = recent_predictions.groupBy(gender_col).agg(\n        count(\"*\").alias(\"count\"),\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\")\n    ).collect()\n\n    for row in gender_analysis:\n        gender_name = getattr(row, gender_col)\n        print(f\"  {gender_name}: Avg Risk={row.avg_risk:.2f}, High-Risk={row.high_risk_pct:.1f}%, N={row['count']:,}\")\n\n    gender_risks = {getattr(row, gender_col): row.avg_risk for row in gender_analysis}\n    if len(gender_risks) == 2:\n        # Use builtins.abs to avoid PySpark function conflict\n        gender_disparity = builtins.abs(list(gender_risks.values())[0] - list(gender_risks.values())[1])\n        print(f\"\\n  Gender disparity: {gender_disparity:.2f} risk points\")\n        if gender_disparity <= 8:\n            print(f\"  ‚úÖ Excellent gender equity\")\n        elif gender_disparity <= 15:\n            print(f\"  ‚úì Acceptable gender equity\")\n        else:\n            print(f\"  ‚ö†Ô∏è  Review for potential gender bias\")\nelse:\n    print(f\"\\n‚öñÔ∏è  Gender Equity: Gender column not available in predictions table\")\n\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compliance & Governance\n",
    "\n",
    "Model governance status, compliance tags, and regulatory readiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPLIANCE & GOVERNANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model governance status\n",
    "try:\n",
    "    champion_info = client.get_model_version_by_alias(full_model_name, \"champion\")\n",
    "    \n",
    "    print(f\"\\nüèõÔ∏è  Model Governance Status:\")\n",
    "    print(f\"  ‚Ä¢ Champion Version: {champion_info.version}\")\n",
    "    print(f\"  ‚Ä¢ Status: {champion_info.status}\")\n",
    "    print(f\"  ‚Ä¢ Created: {champion_info.creation_timestamp}\")\n",
    "    print(f\"  ‚Ä¢ Run ID: {champion_info.run_id}\")\n",
    "    \n",
    "    # Get performance metrics\n",
    "    run_data = client.get_run(champion_info.run_id)\n",
    "    metrics = run_data.data.metrics\n",
    "    \n",
    "    print(f\"\\nüìä Champion Model Metrics:\")\n",
    "    print(f\"  ‚Ä¢ R¬≤ Score: {metrics.get('r2_score', 0):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Mean Absolute Error: {metrics.get('mean_absolute_error', 0):.2f}\")\n",
    "    print(f\"  ‚Ä¢ High-Risk Accuracy: {metrics.get('high_risk_accuracy', 0):.4f}\")\n",
    "    \n",
    "    # Healthcare requirements validation\n",
    "    r2_pass = metrics.get('r2_score', 0) >= 0.70\n",
    "    mae_pass = metrics.get('mean_absolute_error', 999) <= 15.0\n",
    "    hr_acc_pass = metrics.get('high_risk_accuracy', 0) >= 0.60\n",
    "    \n",
    "    print(f\"\\n‚úÖ Healthcare Requirements:\")\n",
    "    print(f\"  {'‚úì' if r2_pass else '‚úó'} R¬≤ Score ‚â• 0.70: {metrics.get('r2_score', 0):.4f}\")\n",
    "    print(f\"  {'‚úì' if mae_pass else '‚úó'} MAE ‚â§ 15.0: {metrics.get('mean_absolute_error', 999):.2f}\")\n",
    "    print(f\"  {'‚úì' if hr_acc_pass else '‚úó'} High-Risk Accuracy ‚â• 0.60: {metrics.get('high_risk_accuracy', 0):.4f}\")\n",
    "    \n",
    "    # Governance tags\n",
    "    if champion_info.tags:\n",
    "        print(f\"\\nüè∑Ô∏è  Governance Tags:\")\n",
    "        compliance_tags = {k: v for k, v in champion_info.tags.items() \n",
    "                          if k in ['healthcare_compliance', 'hipaa_compliant', 'validation_r2', 'model_purpose']}\n",
    "        for key, value in compliance_tags.items():\n",
    "            print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    # Description check\n",
    "    if champion_info.description:\n",
    "        required_terms = ['Healthcare', 'HIPAA', 'Performance']\n",
    "        missing_terms = [term for term in required_terms if term not in champion_info.description]\n",
    "        \n",
    "        if not missing_terms:\n",
    "            print(f\"\\n  ‚úÖ Model description contains all required compliance information\")\n",
    "        else:\n",
    "            print(f\"\\n  ‚ö†Ô∏è  Model description missing terms: {missing_terms}\")\n",
    "    \n",
    "    governance_status = \"COMPLIANT\" if (r2_pass and mae_pass and hr_acc_pass) else \"REQUIRES_REVIEW\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Unable to retrieve champion model: {e}\")\n",
    "    governance_status = \"UNKNOWN\"\n",
    "\n",
    "print(f\"\\nüìã Governance Status: {governance_status}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations & Action Items\n",
    "\n",
    "Prioritized recommendations based on validation findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS & ACTION ITEMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Business recommendations\n",
    "if high_risk_pct < 5:\n",
    "    recommendations.append((\"HIGH\", \"Business\", \"Increase high-risk identification threshold to ensure adequate intervention candidates\"))\n",
    "elif high_risk_pct > 25:\n",
    "    recommendations.append((\"HIGH\", \"Business\", \"Review high-risk threshold to prevent alert fatigue and optimize intervention resources\"))\n",
    "\n",
    "if roi < 50:\n",
    "    recommendations.append((\"HIGH\", \"Business\", \"Optimize intervention strategy to improve ROI and business value\"))\n",
    "\n",
    "# Clinical recommendations\n",
    "if not age_valid:\n",
    "    recommendations.append((\"MEDIUM\", \"Clinical\", \"Review age feature engineering - correlation with risk should be monotonic\"))\n",
    "\n",
    "if \"yes\" in smoker_risks and \"no\" in smoker_risks and smoker_risks[\"yes\"] <= smoker_risks[\"no\"]:\n",
    "    recommendations.append((\"HIGH\", \"Clinical\", \"Smoking feature not properly weighted - recalibrate model\"))\n",
    "\n",
    "# Equity recommendations\n",
    "if len(regional_risks) > 1 and disparity > 25:\n",
    "    recommendations.append((\"MEDIUM\", \"Equity\", \"High regional disparity detected - investigate for potential geographic bias\"))\n",
    "\n",
    "# Technical recommendations\n",
    "if quality_metrics.null_pct > 1:\n",
    "    recommendations.append((\"MEDIUM\", \"Technical\", \"Address data quality issues - null prediction rate above threshold\"))\n",
    "\n",
    "if quality_metrics.outlier_pct > 2:\n",
    "    recommendations.append((\"LOW\", \"Technical\", \"Review outlier predictions to ensure model calibration\"))\n",
    "\n",
    "# Positive findings\n",
    "if not recommendations:\n",
    "    recommendations.append((\"INFO\", \"Success\", \"No critical issues identified - continue monitoring and optimization\"))\n",
    "\n",
    "# Sort and display\n",
    "priority_order = {\"HIGH\": 1, \"MEDIUM\": 2, \"LOW\": 3, \"INFO\": 4}\n",
    "recommendations.sort(key=lambda x: priority_order.get(x[0], 99))\n",
    "\n",
    "print(f\"\\nüìå Prioritized Action Items:\\n\")\n",
    "for priority, category, recommendation in recommendations:\n",
    "    icon = \"üî¥\" if priority == \"HIGH\" else \"üü°\" if priority == \"MEDIUM\" else \"üü¢\" if priority == \"LOW\" else \"‚ÑπÔ∏è\"\n",
    "    print(f\"{icon} [{priority:>6}] {category:>10}: {recommendation}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Report Summary\n",
    "\n",
    "Complete validation scorecard and final assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"FINAL VALIDATION SCORECARD\")\nprint(\"=\"*80)\n\n# Import builtins to avoid conflicts with PySpark functions\nimport builtins\n\n# Calculate overall scores\nvalidation_categories = {\n    \"Technical Validation\": quality_metrics.null_pct < 1 and quality_metrics.outlier_pct < 2,\n    \"Business Performance\": 5 <= high_risk_pct <= 25 and roi > 50,\n    \"Clinical Relevance\": passed_clinical >= 2,\n    \"Regional Equity\": disparity <= 25 if builtins.len(regional_risks) > 1 else True,\n    \"Model Governance\": governance_status == \"COMPLIANT\"\n}\n\npassed_categories = builtins.sum([1 for result in validation_categories.values() if result])\ntotal_categories = builtins.len(validation_categories)\n\nprint(f\"\\nValidation Results by Category:\\n\")\nfor category, result in validation_categories.items():\n    status = \"‚úÖ PASS\" if result else \"‚ö†Ô∏è  REVIEW\"\n    print(f\"  {status:>12} | {category}\")\n\noverall_score = (passed_categories / total_categories) * 100\n\nprint(f\"\\n{'='*80}\")\nprint(f\"OVERALL VALIDATION SCORE: {overall_score:.0f}% ({passed_categories}/{total_categories} categories passed)\")\nprint(f\"{'='*80}\")\n\n# Final assessment\nif overall_score >= 90:\n    final_status = \"EXCELLENT\"\n    icon = \"üü¢\"\n    message = \"Model is performing excellently and ready for production use.\"\nelif overall_score >= 70:\n    final_status = \"GOOD\"\n    icon = \"üü¢\"\n    message = \"Model is performing well with minor areas for improvement.\"\nelif overall_score >= 50:\n    final_status = \"ACCEPTABLE\"\n    icon = \"üü°\"\n    message = \"Model is acceptable but requires attention in several areas.\"\nelse:\n    final_status = \"NEEDS_IMPROVEMENT\"\n    icon = \"üî¥\"\n    message = \"Model requires significant improvement before production use.\"\n\nprint(f\"\\n{icon} FINAL ASSESSMENT: {final_status}\")\nprint(f\"\\n{message}\")\n\nprint(f\"\\nüìä Key Metrics Summary:\")\nprint(f\"  ‚Ä¢ Patients Evaluated: {exec_summary.unique_patients:,}\")\nprint(f\"  ‚Ä¢ High-Risk Identified: {exec_summary.high_risk_count:,}\")\nprint(f\"  ‚Ä¢ Estimated ROI: {roi:.0f}%\")\nprint(f\"  ‚Ä¢ Model Accuracy (R¬≤): {metrics.get('r2_score', 0):.3f}\" if 'metrics' in locals() else \"\")\nprint(f\"  ‚Ä¢ Data Quality: {100 - quality_metrics.null_pct:.1f}%\")\n\nprint(f\"\\nüìÖ Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"üìã Report Period: Last {report_period} days\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"END OF EXECUTIVE VALIDATION REPORT\")\nprint(\"=\"*80)\n\n# Return summary - use builtins to avoid PySpark function conflicts\nreport_summary = {\n    \"status\": final_status,\n    \"overall_score\": overall_score,\n    \"categories_passed\": passed_categories,\n    \"total_categories\": total_categories,\n    \"high_priority_actions\": builtins.len([r for r in recommendations if r[0] == \"HIGH\"]),\n    \"roi\": builtins.round(roi, 2),\n    \"patients_evaluated\": builtins.int(exec_summary.unique_patients),\n    \"high_risk_identified\": builtins.int(exec_summary.high_risk_count),\n    \"report_date\": datetime.now().isoformat()\n}\n\ndbutils.notebook.exit(json.dumps(report_summary))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}