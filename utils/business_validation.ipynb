{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Validation for Healthcare Insurance Risk Model\n",
    "\n",
    "This notebook validates the business value and clinical relevance of the healthcare insurance risk prediction model.\n",
    "\n",
    "## Validation Areas:\n",
    "1. **Business KPIs**: Key performance indicators aligned with business objectives\n",
    "2. **Clinical Relevance**: Medical validity of risk predictions\n",
    "3. **Regional Equity**: Fair predictions across geographic regions\n",
    "4. **Prediction Stability**: Consistency over time\n",
    "5. **ROI Metrics**: Business impact and cost-benefit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Validation Notebook for Healthcare Insurance MLOps\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog\", \"juan_dev\", \"Unity Catalog name\")\n",
    "dbutils.widgets.text(\"ml_schema\", \"healthcare_data\", \"ML Schema name\")\n",
    "dbutils.widgets.text(\"model_name\", \"insurance_model\", \"Model name\")\n",
    "dbutils.widgets.text(\"lookback_days\", \"30\", \"Days to look back for trend analysis\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "ml_schema = dbutils.widgets.get(\"ml_schema\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "lookback_days = int(dbutils.widgets.get(\"lookback_days\"))\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "print(f\"=\"*80)\n",
    "print(f\"BUSINESS VALIDATION REPORT\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"Model: {catalog}.{ml_schema}.{model_name}\")\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Lookback period: {lookback_days} days\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business KPI Validation\n",
    "\n",
    "Validate that the model meets key business objectives and performance indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load predictions data\npredictions_table = f\"{catalog}.{ml_schema}.ml_patient_predictions\"\n\ntry:\n    predictions_df = spark.table(predictions_table)\n    \n    # Filter for recent predictions\n    cutoff_date = (datetime.now() - timedelta(days=lookback_days)).strftime('%Y-%m-%d')\n    recent_predictions = predictions_df.filter(\n        col(\"prediction_timestamp\") >= cutoff_date\n    )\n    \n    total_predictions = recent_predictions.count()\n    print(f\"\\nüìä KPI Summary (Last {lookback_days} days)\")\n    print(f\"-\" * 80)\n    print(f\"Total predictions: {total_predictions:,}\")\n    \n    if total_predictions == 0:\n        print(f\"\\n‚ö†Ô∏è  No predictions found in the last {lookback_days} days\")\n        dbutils.notebook.exit(json.dumps({\"status\": \"NO_DATA\", \"message\": \"No recent predictions\"}))\n    \n    # Calculate key business metrics (using actual column names from schema)\n    business_metrics = recent_predictions.agg(\n        avg(\"adjusted_prediction\").alias(\"avg_risk_score\"),\n        stddev(\"adjusted_prediction\").alias(\"risk_score_std\"),\n        min(\"adjusted_prediction\").alias(\"min_risk\"),\n        max(\"adjusted_prediction\").alias(\"max_risk\"),\n        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\"),\n        avg(\"prediction_lower_bound\").alias(\"avg_ci_lower\"),\n        avg(\"prediction_upper_bound\").alias(\"avg_ci_upper\")\n    ).collect()[0]\n    \n    print(f\"\\nRisk Score Statistics:\")\n    print(f\"  Average: {business_metrics.avg_risk_score:.2f}\")\n    print(f\"  Std Dev: {business_metrics.risk_score_std:.2f}\")\n    print(f\"  Range: {business_metrics.min_risk:.2f} - {business_metrics.max_risk:.2f}\")\n    print(f\"  High-risk patients: {business_metrics.high_risk_pct:.1f}%\")\n    \n    # Business KPI Targets\n    kpi_targets = {\n        \"high_risk_pct_target\": (5.0, 25.0),  # Target range: 5-25%\n        \"avg_risk_target\": (20.0, 60.0),       # Average risk should be moderate\n        \"prediction_volume_min\": 100           # Minimum predictions for statistical validity\n    }\n    \n    print(f\"\\n‚úÖ KPI Target Validation:\")\n    kpi_results = {}\n    \n    # High-risk percentage check\n    if kpi_targets[\"high_risk_pct_target\"][0] <= business_metrics.high_risk_pct <= kpi_targets[\"high_risk_pct_target\"][1]:\n        print(f\"  ‚úÖ High-risk %: {business_metrics.high_risk_pct:.1f}% (Target: {kpi_targets['high_risk_pct_target'][0]}-{kpi_targets['high_risk_pct_target'][1]}%)\")\n        kpi_results[\"high_risk_pct\"] = \"PASS\"\n    else:\n        print(f\"  ‚ùå High-risk %: {business_metrics.high_risk_pct:.1f}% (Target: {kpi_targets['high_risk_pct_target'][0]}-{kpi_targets['high_risk_pct_target'][1]}%)\")\n        kpi_results[\"high_risk_pct\"] = \"FAIL\"\n    \n    # Average risk check\n    if kpi_targets[\"avg_risk_target\"][0] <= business_metrics.avg_risk_score <= kpi_targets[\"avg_risk_target\"][1]:\n        print(f\"  ‚úÖ Avg risk: {business_metrics.avg_risk_score:.2f} (Target: {kpi_targets['avg_risk_target'][0]}-{kpi_targets['avg_risk_target'][1]})\")\n        kpi_results[\"avg_risk\"] = \"PASS\"\n    else:\n        print(f\"  ‚ùå Avg risk: {business_metrics.avg_risk_score:.2f} (Target: {kpi_targets['avg_risk_target'][0]}-{kpi_targets['avg_risk_target'][1]})\")\n        kpi_results[\"avg_risk\"] = \"FAIL\"\n    \n    # Prediction volume check\n    if total_predictions >= kpi_targets[\"prediction_volume_min\"]:\n        print(f\"  ‚úÖ Prediction volume: {total_predictions:,} (Minimum: {kpi_targets['prediction_volume_min']:,})\")\n        kpi_results[\"volume\"] = \"PASS\"\n    else:\n        print(f\"  ‚ùå Prediction volume: {total_predictions:,} (Minimum: {kpi_targets['prediction_volume_min']:,})\")\n        kpi_results[\"volume\"] = \"FAIL\"\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå KPI validation failed: {str(e)}\")\n    import traceback\n    traceback.print_exc()\n    dbutils.notebook.exit(json.dumps({\"status\": \"ERROR\", \"message\": str(e)}))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clinical Relevance Validation\n",
    "\n",
    "Validate that predictions align with medical knowledge and healthcare best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\nüè• Clinical Relevance Validation\")\nprint(f\"-\" * 80)\n\nclinical_results = {}\n\ntry:\n    # 1. Smoking Impact Validation\n    print(f\"\\n1. Smoking Impact Analysis:\")\n    smoking_analysis = recent_predictions.groupBy(\"patient_smoking_status\").agg(\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\"),\n        count(\"*\").alias(\"patient_count\")\n    ).orderBy(\"patient_smoking_status\").collect()\n    \n    for row in smoking_analysis:\n        print(f\"   {row.patient_smoking_status}: Avg Risk={row.avg_risk:.2f}, High-Risk%={row.high_risk_pct:.1f}%, N={row.patient_count:,}\")\n    \n    # Validate that smokers have higher average risk\n    smoker_risks = {row.patient_smoking_status: row.avg_risk for row in smoking_analysis}\n    if \"yes\" in smoker_risks and \"no\" in smoker_risks:\n        if smoker_risks[\"yes\"] > smoker_risks[\"no\"]:\n            print(f\"   ‚úÖ Smokers have higher average risk (clinically valid)\")\n            clinical_results[\"smoking_correlation\"] = \"PASS\"\n        else:\n            print(f\"   ‚ùå Smokers do NOT have higher risk (clinically invalid)\")\n            clinical_results[\"smoking_correlation\"] = \"FAIL\"\n    \n    # 2. Age Correlation Validation\n    print(f\"\\n2. Age Correlation Analysis:\")\n    age_analysis = recent_predictions.groupBy(\"patient_age_category\").agg(\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        count(\"*\").alias(\"patient_count\")\n    ).orderBy(\"patient_age_category\").collect()\n    \n    prev_risk = 0\n    age_monotonic = True\n    for row in age_analysis:\n        print(f\"   {row.patient_age_category}: Avg Risk={row.avg_risk:.2f}, N={row.patient_count:,}\")\n        if row.avg_risk < prev_risk:\n            age_monotonic = False\n        prev_risk = row.avg_risk\n    \n    if age_monotonic:\n        print(f\"   ‚úÖ Age positively correlated with risk (clinically valid)\")\n        clinical_results[\"age_correlation\"] = \"PASS\"\n    else:\n        print(f\"   ‚ö†Ô∏è  Age NOT consistently correlated with risk\")\n        clinical_results[\"age_correlation\"] = \"WARN\"\n    \n    # 3. BMI Impact Validation\n    print(f\"\\n3. BMI Category Impact Analysis:\")\n    # Derive BMI category from numeric bmi field\n    bmi_predictions = recent_predictions.withColumn(\n        \"bmi_category\",\n        when(col(\"bmi\") < 18.5, \"underweight\")\n        .when((col(\"bmi\") >= 18.5) & (col(\"bmi\") < 25), \"normal\")\n        .when((col(\"bmi\") >= 25) & (col(\"bmi\") < 30), \"overweight\")\n        .when(col(\"bmi\") >= 30, \"obese\")\n        .otherwise(\"unknown\")\n    )\n    \n    bmi_analysis = bmi_predictions.groupBy(\"bmi_category\").agg(\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        count(\"*\").alias(\"patient_count\")\n    ).orderBy(\"bmi_category\").collect()\n    \n    for row in bmi_analysis:\n        print(f\"   {row.bmi_category}: Avg Risk={row.avg_risk:.2f}, N={row.patient_count:,}\")\n    \n    # Validate that obese patients have higher risk than normal\n    bmi_risks = {row.bmi_category: row.avg_risk for row in bmi_analysis}\n    if \"obese\" in bmi_risks and \"normal\" in bmi_risks:\n        if bmi_risks[\"obese\"] > bmi_risks[\"normal\"]:\n            print(f\"   ‚úÖ Obese patients have higher risk than normal BMI (clinically valid)\")\n            clinical_results[\"bmi_correlation\"] = \"PASS\"\n        else:\n            print(f\"   ‚ùå Obese patients do NOT have higher risk (clinically questionable)\")\n            clinical_results[\"bmi_correlation\"] = \"FAIL\"\n    \n    # 4. Risk Category Distribution\n    print(f\"\\n4. Risk Category Distribution:\")\n    risk_dist = recent_predictions.groupBy(\"risk_category\").agg(\n        count(\"*\").alias(\"patient_count\")\n    ).withColumn(\"percentage\", col(\"patient_count\") / total_predictions * 100).orderBy(\"risk_category\").collect()\n    \n    for row in risk_dist:\n        print(f\"   {row.risk_category}: {row.patient_count:,} ({row.percentage:.1f}%)\")\n    \n    # Healthcare industry benchmark: Expect majority in low/medium risk\n    risk_pcts = {row.risk_category: row.percentage for row in risk_dist}\n    low_medium_pct = risk_pcts.get(\"low\", 0) + risk_pcts.get(\"medium\", 0)\n    \n    if low_medium_pct >= 60:\n        print(f\"   ‚úÖ Low/Medium risk patients: {low_medium_pct:.1f}% (Expected >60%)\")\n        clinical_results[\"risk_distribution\"] = \"PASS\"\n    else:\n        print(f\"   ‚ö†Ô∏è  Low/Medium risk patients: {low_medium_pct:.1f}% (Expected >60%)\")\n        clinical_results[\"risk_distribution\"] = \"WARN\"\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Clinical validation failed: {str(e)}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regional Equity Validation\n",
    "\n",
    "Ensure fair and equitable predictions across all geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\nüåé Regional Equity Validation\")\nprint(f\"-\" * 80)\n\nequity_results = {}\n\ntry:\n    # Regional distribution analysis\n    regional_analysis = recent_predictions.groupBy(\"region\").agg(\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        stddev(\"adjusted_prediction\").alias(\"risk_std\"),\n        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\"),\n        count(\"*\").alias(\"patient_count\")\n    ).orderBy(\"region\").collect()\n    \n    print(f\"\\nRisk Metrics by Region:\")\n    print(f\"{'Region':<15} {'Avg Risk':<12} {'Std Dev':<12} {'High-Risk %':<12} {'Count':<10}\")\n    print(f\"-\" * 80)\n    \n    regional_risks = []\n    for row in regional_analysis:\n        regional_risks.append(row.avg_risk)\n        print(f\"{row.region:<15} {row.avg_risk:<12.2f} {row.risk_std:<12.2f} {row.high_risk_pct:<12.1f} {row.patient_count:<10,}\")\n    \n    # Calculate regional disparity using Python's built-in max/min\n    if len(regional_risks) > 1:\n        import builtins\n        max_regional_risk = builtins.max(regional_risks)\n        min_regional_risk = builtins.min(regional_risks)\n        regional_disparity = ((max_regional_risk - min_regional_risk) / min_regional_risk) * 100\n        \n        print(f\"\\nRegional Disparity Analysis:\")\n        print(f\"  Max regional risk: {max_regional_risk:.2f}\")\n        print(f\"  Min regional risk: {min_regional_risk:.2f}\")\n        print(f\"  Disparity: {regional_disparity:.1f}%\")\n        \n        # Acceptable disparity threshold: 20%\n        if regional_disparity <= 20:\n            print(f\"  ‚úÖ Regional disparity within acceptable range (<20%)\")\n            equity_results[\"regional_equity\"] = \"PASS\"\n        else:\n            print(f\"  ‚ö†Ô∏è  High regional disparity (>{regional_disparity:.1f}%) - investigate regional bias\")\n            equity_results[\"regional_equity\"] = \"WARN\"\n    \n    # Gender equity analysis\n    print(f\"\\nGender Equity Analysis:\")\n    gender_analysis = recent_predictions.groupBy(\"sex\").agg(\n        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\"),\n        count(\"*\").alias(\"patient_count\")\n    ).collect()\n    \n    for row in gender_analysis:\n        print(f\"  {row.sex}: Avg Risk={row.avg_risk:.2f}, High-Risk%={row.high_risk_pct:.1f}%, N={row.patient_count:,}\")\n    \n    # Check for extreme gender bias\n    gender_risks = {row.sex: row.avg_risk for row in gender_analysis}\n    if len(gender_risks) == 2:\n        gender_disparity = builtins.abs(list(gender_risks.values())[0] - list(gender_risks.values())[1])\n        if gender_disparity <= 10:\n            print(f\"  ‚úÖ Gender disparity acceptable: {gender_disparity:.2f} points\")\n            equity_results[\"gender_equity\"] = \"PASS\"\n        else:\n            print(f\"  ‚ö†Ô∏è  High gender disparity: {gender_disparity:.2f} points\")\n            equity_results[\"gender_equity\"] = \"WARN\"\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Equity validation failed: {str(e)}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction Stability Over Time\n",
    "\n",
    "Validate that predictions are consistent and stable over the lookback period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìà Prediction Stability Analysis\")\n",
    "print(f\"-\" * 80)\n",
    "\n",
    "stability_results = {}\n",
    "\n",
    "try:\n",
    "    # Daily trend analysis\n",
    "    daily_trends = recent_predictions.withColumn(\n",
    "        \"prediction_date\", date_format(col(\"prediction_timestamp\"), \"yyyy-MM-dd\")\n",
    "    ).groupBy(\"prediction_date\").agg(\n",
    "        avg(\"adjusted_prediction\").alias(\"avg_risk\"),\n",
    "        count(\"*\").alias(\"prediction_count\"),\n",
    "        (sum(when(col(\"high_risk_patient\") == True, 1).otherwise(0)) / count(\"*\") * 100).alias(\"high_risk_pct\")\n",
    "    ).orderBy(\"prediction_date\").collect()\n",
    "    \n",
    "    if len(daily_trends) > 1:\n",
    "        print(f\"\\nDaily Prediction Trends (Last {min(7, len(daily_trends))} days):\")\n",
    "        print(f\"{'Date':<12} {'Avg Risk':<12} {'High-Risk %':<12} {'Count':<10}\")\n",
    "        print(f\"-\" * 80)\n",
    "        \n",
    "        # Show last 7 days\n",
    "        for row in daily_trends[-7:]:\n",
    "            print(f\"{row.prediction_date:<12} {row.avg_risk:<12.2f} {row.high_risk_pct:<12.1f} {row.prediction_count:<10,}\")\n",
    "        \n",
    "        # Calculate coefficient of variation (CV) for stability\n",
    "        daily_risks = [row.avg_risk for row in daily_trends]\n",
    "        avg_daily_risk = sum(daily_risks) / len(daily_risks)\n",
    "        variance = sum([(x - avg_daily_risk)**2 for x in daily_risks]) / len(daily_risks)\n",
    "        std_dev = variance ** 0.5\n",
    "        cv = (std_dev / avg_daily_risk) * 100\n",
    "        \n",
    "        print(f\"\\nStability Metrics:\")\n",
    "        print(f\"  Coefficient of Variation: {cv:.2f}%\")\n",
    "        \n",
    "        # CV < 10% indicates good stability\n",
    "        if cv < 10:\n",
    "            print(f\"  ‚úÖ High prediction stability (CV < 10%)\")\n",
    "            stability_results[\"temporal_stability\"] = \"PASS\"\n",
    "        elif cv < 20:\n",
    "            print(f\"  ‚ö†Ô∏è  Moderate prediction stability (CV < 20%)\")\n",
    "            stability_results[\"temporal_stability\"] = \"WARN\"\n",
    "        else:\n",
    "            print(f\"  ‚ùå Low prediction stability (CV > 20%)\")\n",
    "            stability_results[\"temporal_stability\"] = \"FAIL\"\n",
    "    else:\n",
    "        print(f\"  ‚ÑπÔ∏è  Insufficient data for temporal stability analysis\")\n",
    "        stability_results[\"temporal_stability\"] = \"INSUFFICIENT_DATA\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Stability validation failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Business Impact and ROI Metrics\n",
    "\n",
    "Calculate the business value and return on investment of the risk prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüí∞ Business Impact & ROI Analysis\")\n",
    "print(f\"-\" * 80)\n",
    "\n",
    "roi_results = {}\n",
    "\n",
    "try:\n",
    "    # Business assumptions (adjust based on actual business metrics)\n",
    "    assumptions = {\n",
    "        \"avg_intervention_cost\": 500,        # Cost per high-risk patient intervention\n",
    "        \"avg_prevented_claim\": 5000,         # Average claim prevented by early intervention\n",
    "        \"intervention_success_rate\": 0.30,   # 30% of interventions prevent a claim\n",
    "        \"model_operating_cost_monthly\": 2000 # Monthly cost to run the model\n",
    "    }\n",
    "    \n",
    "    # Calculate high-risk patient counts\n",
    "    high_risk_patients = recent_predictions.filter(col(\"high_risk_patient\") == True).count()\n",
    "    \n",
    "    # ROI Calculation\n",
    "    print(f\"\\nBusiness Impact Estimates:\")\n",
    "    print(f\"  High-risk patients identified: {high_risk_patients:,}\")\n",
    "    \n",
    "    # Calculate costs\n",
    "    intervention_costs = high_risk_patients * assumptions[\"avg_intervention_cost\"]\n",
    "    model_costs = assumptions[\"model_operating_cost_monthly\"] * (lookback_days / 30)\n",
    "    total_costs = intervention_costs + model_costs\n",
    "    \n",
    "    print(f\"\\nCosts:\")\n",
    "    print(f\"  Intervention costs: ${intervention_costs:,.2f}\")\n",
    "    print(f\"  Model operating costs: ${model_costs:,.2f}\")\n",
    "    print(f\"  Total costs: ${total_costs:,.2f}\")\n",
    "    \n",
    "    # Calculate benefits\n",
    "    successful_interventions = high_risk_patients * assumptions[\"intervention_success_rate\"]\n",
    "    prevented_claims_value = successful_interventions * assumptions[\"avg_prevented_claim\"]\n",
    "    \n",
    "    print(f\"\\nBenefits:\")\n",
    "    print(f\"  Estimated successful interventions: {successful_interventions:.0f}\")\n",
    "    print(f\"  Value of prevented claims: ${prevented_claims_value:,.2f}\")\n",
    "    \n",
    "    # Calculate ROI\n",
    "    net_benefit = prevented_claims_value - total_costs\n",
    "    roi_percentage = (net_benefit / total_costs) * 100 if total_costs > 0 else 0\n",
    "    \n",
    "    print(f\"\\nROI Summary:\")\n",
    "    print(f\"  Net benefit: ${net_benefit:,.2f}\")\n",
    "    print(f\"  ROI: {roi_percentage:.1f}%\")\n",
    "    \n",
    "    if roi_percentage > 100:\n",
    "        print(f\"  ‚úÖ Strong positive ROI (>{roi_percentage:.0f}%)\")\n",
    "        roi_results[\"roi_status\"] = \"EXCELLENT\"\n",
    "    elif roi_percentage > 50:\n",
    "        print(f\"  ‚úÖ Positive ROI ({roi_percentage:.0f}%)\")\n",
    "        roi_results[\"roi_status\"] = \"GOOD\"\n",
    "    elif roi_percentage > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Marginal ROI ({roi_percentage:.0f}%)\")\n",
    "        roi_results[\"roi_status\"] = \"MARGINAL\"\n",
    "    else:\n",
    "        print(f\"  ‚ùå Negative ROI ({roi_percentage:.0f}%)\")\n",
    "        roi_results[\"roi_status\"] = \"NEGATIVE\"\n",
    "    \n",
    "    print(f\"\\nüìù Note: ROI calculations based on business assumptions. Adjust assumptions for actual metrics.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ROI validation failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Governance Status\n",
    "\n",
    "Check current model governance and champion model status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüèõÔ∏è  Model Governance Status\")\n",
    "print(f\"-\" * 80)\n",
    "\n",
    "governance_results = {}\n",
    "\n",
    "try:\n",
    "    full_model_name = f\"{catalog}.{ml_schema}.{model_name}\"\n",
    "    \n",
    "    # Check champion model\n",
    "    try:\n",
    "        champion_info = client.get_model_version_by_alias(full_model_name, \"champion\")\n",
    "        print(f\"\\nChampion Model:\")\n",
    "        print(f\"  Version: {champion_info.version}\")\n",
    "        print(f\"  Status: {champion_info.status}\")\n",
    "        print(f\"  Run ID: {champion_info.run_id}\")\n",
    "        \n",
    "        # Get model metrics\n",
    "        run_data = client.get_run(champion_info.run_id)\n",
    "        metrics = run_data.data.metrics\n",
    "        \n",
    "        print(f\"\\n  Performance Metrics:\")\n",
    "        for metric_name in [\"r2_score\", \"mean_absolute_error\", \"high_risk_accuracy\"]:\n",
    "            if metric_name in metrics:\n",
    "                print(f\"    {metric_name}: {metrics[metric_name]:.4f}\")\n",
    "        \n",
    "        # Check governance tags\n",
    "        if champion_info.tags:\n",
    "            print(f\"\\n  Governance Tags:\")\n",
    "            for key, value in champion_info.tags.items():\n",
    "                if key in [\"healthcare_compliance\", \"validation_r2\", \"hipaa_compliant\"]:\n",
    "                    print(f\"    {key}: {value}\")\n",
    "        \n",
    "        governance_results[\"champion_exists\"] = True\n",
    "        governance_results[\"champion_version\"] = champion_info.version\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  No champion model found: {e}\")\n",
    "        governance_results[\"champion_exists\"] = False\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Governance check failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Business Validation Summary\n",
    "\n",
    "Comprehensive summary of all business validation checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BUSINESS VALIDATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Collect all validation results\n",
    "all_results = {\n",
    "    \"Business KPIs\": kpi_results,\n",
    "    \"Clinical Relevance\": clinical_results,\n",
    "    \"Regional Equity\": equity_results,\n",
    "    \"Temporal Stability\": stability_results,\n",
    "    \"ROI\": roi_results,\n",
    "    \"Governance\": governance_results\n",
    "}\n",
    "\n",
    "total_checks = 0\n",
    "passed_checks = 0\n",
    "warnings = 0\n",
    "failed_checks = 0\n",
    "\n",
    "for category, results in all_results.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for check_name, result in results.items():\n",
    "        total_checks += 1\n",
    "        if result == \"PASS\" or result == True or result in [\"EXCELLENT\", \"GOOD\"]:\n",
    "            print(f\"  ‚úÖ {check_name}: {result}\")\n",
    "            passed_checks += 1\n",
    "        elif result == \"WARN\" or result == \"MARGINAL\":\n",
    "            print(f\"  ‚ö†Ô∏è  {check_name}: {result}\")\n",
    "            warnings += 1\n",
    "        elif result == False or result == \"FAIL\" or result == \"NEGATIVE\":\n",
    "            print(f\"  ‚ùå {check_name}: {result}\")\n",
    "            failed_checks += 1\n",
    "        else:\n",
    "            print(f\"  ‚ÑπÔ∏è  {check_name}: {result}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Validation Score: {passed_checks}/{total_checks} checks passed\")\n",
    "print(f\"Warnings: {warnings}\")\n",
    "print(f\"Failures: {failed_checks}\")\n",
    "\n",
    "if failed_checks == 0 and warnings == 0:\n",
    "    overall_status = \"EXCELLENT\"\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ OVERALL STATUS: EXCELLENT - Model meets all business objectives ‚úÖ‚úÖ‚úÖ\")\n",
    "elif failed_checks == 0:\n",
    "    overall_status = \"GOOD\"\n",
    "    print(f\"\\n‚úÖ OVERALL STATUS: GOOD - Model meets business objectives with minor warnings ‚úÖ\")\n",
    "elif failed_checks <= 2:\n",
    "    overall_status = \"ACCEPTABLE\"\n",
    "    print(f\"\\n‚ö†Ô∏è  OVERALL STATUS: ACCEPTABLE - Model has some issues to address ‚ö†Ô∏è\")\n",
    "else:\n",
    "    overall_status = \"NEEDS_IMPROVEMENT\"\n",
    "    print(f\"\\n‚ùå OVERALL STATUS: NEEDS IMPROVEMENT - Model requires attention ‚ùå\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create summary for return\n",
    "summary = {\n",
    "    \"status\": overall_status,\n",
    "    \"total_checks\": total_checks,\n",
    "    \"passed\": passed_checks,\n",
    "    \"warnings\": warnings,\n",
    "    \"failures\": failed_checks,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"lookback_days\": lookback_days,\n",
    "    \"total_predictions\": total_predictions\n",
    "}\n",
    "\n",
    "dbutils.notebook.exit(json.dumps(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}